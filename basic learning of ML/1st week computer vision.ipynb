{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01078061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da77659d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\Shaon\\AdobeStock_458083018-1024x683.jpeg: 448x640 5 persons, 2 buss, 5 backpacks, 1 handbag, 1 book, 408.2ms\n",
      "Speed: 5.0ms preprocess, 408.2ms inference, 786.9ms postprocess per image at shape (1, 3, 448, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    " \n",
    "model = YOLO('yolov8l.pt')\n",
    "results = model(\"AdobeStock_458083018-1024x683.jpeg\", show=True)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b36e3daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\Shaon\\AdobeStock_458083018-1024x683.jpeg: 448x640 5 persons, 2 buss, 5 backpacks, 1 handbag, 1 book, 390.8ms\n",
      "Speed: 3.0ms preprocess, 390.8ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "model = YOLO('yolov8l.pt')\n",
    "results = model(\"AdobeStock_458083018-1024x683.jpeg\", show=True)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df6d2052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\Shaon\\zzz.webp: 384x640 9 persons, 17 cars, 355.4ms\n",
      "Speed: 2.1ms preprocess, 355.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    " \n",
    "from pathlib import Path\n",
    "# Specify the directory to save the YOLO weights file\n",
    "weights_dir = Path('path/to/your/weights/directory')\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO(weights_dir / 'yolov8l.pt')\n",
    "results = model(\"zzz.webp\", show=True)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0f1709",
   "metadata": {},
   "source": [
    "# YoLo with web cma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95b389c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 15 persons, 11 bicycles, 3 traffic lights, 2 backpacks, 567.5ms\n",
      "Speed: 5.4ms preprocess, 567.5ms inference, 1321.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "5.833817156008939e-10\n",
      "\n",
      "0: 384x640 15 persons, 11 bicycles, 1 car, 3 traffic lights, 3 backpacks, 520.2ms\n",
      "Speed: 3.0ms preprocess, 520.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "0.2063945592156906\n",
      "\n",
      "0: 384x640 15 persons, 11 bicycles, 1 car, 1 traffic light, 2 backpacks, 519.1ms\n",
      "Speed: 2.0ms preprocess, 519.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1.7304162351941317\n",
      "\n",
      "0: 384x640 16 persons, 11 bicycles, 1 car, 2 traffic lights, 2 backpacks, 511.5ms\n",
      "Speed: 2.4ms preprocess, 511.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1.7733138567626472\n",
      "\n",
      "0: 384x640 15 persons, 13 bicycles, 2 traffic lights, 2 backpacks, 507.6ms\n",
      "Speed: 2.8ms preprocess, 507.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1.7750256670252573\n",
      "\n",
      "0: 384x640 16 persons, 11 bicycles, 2 traffic lights, 2 backpacks, 507.1ms\n",
      "Speed: 2.9ms preprocess, 507.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1.8026464404396179\n",
      "\n",
      "0: 384x640 14 persons, 11 bicycles, 1 traffic light, 2 backpacks, 1 handbag, 508.2ms\n",
      "Speed: 3.6ms preprocess, 508.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1.8175800203758747\n",
      "\n",
      "0: 384x640 14 persons, 13 bicycles, 1 traffic light, 2 backpacks, 516.4ms\n",
      "Speed: 2.4ms preprocess, 516.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1.8148580844033904\n",
      "\n",
      "0: 384x640 15 persons, 14 bicycles, 3 traffic lights, 2 backpacks, 512.6ms\n",
      "Speed: 2.0ms preprocess, 512.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1.779539746113638\n",
      "\n",
      "0: 384x640 13 persons, 14 bicycles, 2 traffic lights, 2 backpacks, 1 handbag, 412.7ms\n",
      "Speed: 2.3ms preprocess, 412.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "1.7893292208193885\n",
      "\n",
      "0: 384x640 15 persons, 16 bicycles, 2 traffic lights, 2 backpacks, 2 handbags, 348.4ms\n",
      "Speed: 2.0ms preprocess, 348.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.1525864834141224\n",
      "\n",
      "0: 384x640 15 persons, 14 bicycles, 2 traffic lights, 2 backpacks, 333.6ms\n",
      "Speed: 2.0ms preprocess, 333.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.51624371359615\n",
      "\n",
      "0: 384x640 17 persons, 12 bicycles, 2 traffic lights, 2 backpacks, 323.0ms\n",
      "Speed: 2.0ms preprocess, 323.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.6591112973607984\n",
      "\n",
      "0: 384x640 16 persons, 14 bicycles, 2 traffic lights, 3 backpacks, 298.1ms\n",
      "Speed: 2.0ms preprocess, 298.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.5729574413136467\n",
      "\n",
      "0: 384x640 17 persons, 13 bicycles, 2 traffic lights, 3 backpacks, 1 handbag, 334.7ms\n",
      "Speed: 2.0ms preprocess, 334.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.7198421905528325\n",
      "\n",
      "0: 384x640 17 persons, 14 bicycles, 2 traffic lights, 3 backpacks, 1 handbag, 313.8ms\n",
      "Speed: 2.0ms preprocess, 313.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.468124720119996\n",
      "\n",
      "0: 384x640 15 persons, 15 bicycles, 2 traffic lights, 3 backpacks, 300.0ms\n",
      "Speed: 1.0ms preprocess, 300.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.575104218468925\n",
      "\n",
      "0: 384x640 14 persons, 13 bicycles, 2 traffic lights, 2 backpacks, 334.0ms\n",
      "Speed: 2.0ms preprocess, 334.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.7191862481644877\n",
      "\n",
      "0: 384x640 15 persons, 13 bicycles, 2 traffic lights, 2 backpacks, 330.0ms\n",
      "Speed: 2.0ms preprocess, 330.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.5322234377301718\n",
      "\n",
      "0: 384x640 14 persons, 13 bicycles, 2 traffic lights, 2 backpacks, 1 handbag, 338.9ms\n",
      "Speed: 1.2ms preprocess, 338.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.314736029395127\n",
      "\n",
      "0: 384x640 15 persons, 13 bicycles, 2 traffic lights, 1 backpack, 1 handbag, 348.8ms\n",
      "Speed: 2.0ms preprocess, 348.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.2608304742602567\n",
      "\n",
      "0: 384x640 15 persons, 14 bicycles, 2 traffic lights, 1 backpack, 1 handbag, 342.4ms\n",
      "Speed: 1.0ms preprocess, 342.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.171466083785387\n",
      "\n",
      "0: 384x640 15 persons, 17 bicycles, 2 traffic lights, 1 backpack, 1 handbag, 352.5ms\n",
      "Speed: 2.0ms preprocess, 352.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.4182900342422116\n",
      "\n",
      "0: 384x640 15 persons, 15 bicycles, 2 traffic lights, 2 backpacks, 1 handbag, 321.6ms\n",
      "Speed: 2.0ms preprocess, 321.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.3042752360007843\n",
      "\n",
      "0: 384x640 14 persons, 14 bicycles, 2 traffic lights, 2 backpacks, 1 handbag, 332.1ms\n",
      "Speed: 2.0ms preprocess, 332.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.4740865006305692\n",
      "\n",
      "0: 384x640 15 persons, 14 bicycles, 1 car, 2 traffic lights, 1 backpack, 1 handbag, 332.4ms\n",
      "Speed: 2.0ms preprocess, 332.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.4877174148356618\n",
      "\n",
      "0: 384x640 16 persons, 14 bicycles, 1 car, 2 traffic lights, 2 backpacks, 1 handbag, 306.5ms\n",
      "Speed: 1.0ms preprocess, 306.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.476398866158394\n",
      "\n",
      "0: 384x640 13 persons, 13 bicycles, 2 cars, 2 traffic lights, 1 handbag, 322.9ms\n",
      "Speed: 2.0ms preprocess, 322.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.572395668826529\n",
      "\n",
      "0: 384x640 14 persons, 11 bicycles, 1 car, 2 traffic lights, 1 handbag, 347.3ms\n",
      "Speed: 2.0ms preprocess, 347.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.3578686052930706\n",
      "\n",
      "0: 384x640 13 persons, 10 bicycles, 3 cars, 2 traffic lights, 1 backpack, 1 handbag, 304.6ms\n",
      "Speed: 2.0ms preprocess, 304.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.2554981175968263\n",
      "\n",
      "0: 384x640 14 persons, 10 bicycles, 2 cars, 2 traffic lights, 1 backpack, 1 handbag, 329.0ms\n",
      "Speed: 2.0ms preprocess, 329.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.503906623111162\n",
      "\n",
      "0: 384x640 13 persons, 12 bicycles, 3 cars, 2 traffic lights, 1 backpack, 1 handbag, 334.0ms\n",
      "Speed: 2.0ms preprocess, 334.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.3594178049441803\n",
      "\n",
      "0: 384x640 14 persons, 13 bicycles, 3 cars, 2 traffic lights, 1 backpack, 1 handbag, 317.0ms\n",
      "Speed: 1.0ms preprocess, 317.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.297331093497447\n",
      "\n",
      "0: 384x640 14 persons, 13 bicycles, 3 cars, 2 traffic lights, 1 backpack, 1 handbag, 330.7ms\n",
      "Speed: 3.0ms preprocess, 330.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.599066907179634\n",
      "\n",
      "0: 384x640 13 persons, 11 bicycles, 3 cars, 2 traffic lights, 2 backpacks, 1 handbag, 331.6ms\n",
      "Speed: 2.0ms preprocess, 331.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.49821700749113\n",
      "\n",
      "0: 384x640 15 persons, 11 bicycles, 2 cars, 3 traffic lights, 1 backpack, 1 handbag, 323.0ms\n",
      "Speed: 2.0ms preprocess, 323.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.522445066824313\n",
      "\n",
      "0: 384x640 15 persons, 12 bicycles, 3 cars, 4 traffic lights, 1 backpack, 1 handbag, 316.1ms\n",
      "Speed: 2.0ms preprocess, 316.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.547371975594617\n",
      "\n",
      "0: 384x640 15 persons, 12 bicycles, 3 cars, 2 traffic lights, 1 backpack, 1 handbag, 330.1ms\n",
      "Speed: 2.0ms preprocess, 330.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.3390977447802137\n",
      "\n",
      "0: 384x640 16 persons, 11 bicycles, 2 cars, 2 traffic lights, 1 backpack, 1 handbag, 330.6ms\n",
      "Speed: 2.0ms preprocess, 330.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.2921572806514194\n",
      "\n",
      "0: 384x640 13 persons, 11 bicycles, 2 cars, 2 traffic lights, 1 backpack, 1 handbag, 295.8ms\n",
      "Speed: 2.0ms preprocess, 295.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.3135996593286636\n",
      "\n",
      "0: 384x640 14 persons, 10 bicycles, 2 cars, 2 traffic lights, 1 backpack, 1 handbag, 319.4ms\n",
      "Speed: 2.0ms preprocess, 319.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5559875488430026\n",
      "\n",
      "0: 384x640 14 persons, 10 bicycles, 4 cars, 2 traffic lights, 1 backpack, 1 handbag, 329.1ms\n",
      "Speed: 2.0ms preprocess, 329.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.4073857880402807\n",
      "\n",
      "0: 384x640 13 persons, 9 bicycles, 3 cars, 3 traffic lights, 1 handbag, 307.9ms\n",
      "Speed: 2.0ms preprocess, 307.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.324906849719025\n",
      "\n",
      "0: 384x640 15 persons, 10 bicycles, 1 car, 2 traffic lights, 2 handbags, 1 suitcase, 316.4ms\n",
      "Speed: 2.0ms preprocess, 316.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.493818821794658\n",
      "\n",
      "0: 384x640 14 persons, 9 bicycles, 3 cars, 2 traffic lights, 2 handbags, 321.8ms\n",
      "Speed: 2.0ms preprocess, 321.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.418854858466301\n",
      "\n",
      "0: 384x640 14 persons, 9 bicycles, 3 cars, 2 traffic lights, 2 handbags, 300.3ms\n",
      "Speed: 2.0ms preprocess, 300.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.3966725579168173\n",
      "\n",
      "0: 384x640 15 persons, 8 bicycles, 3 cars, 2 traffic lights, 1 handbag, 327.8ms\n",
      "Speed: 2.0ms preprocess, 327.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2.5341450387767384\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import cvzone\n",
    "import math\n",
    "import time\n",
    "\n",
    "#cap = cv2.VideoCapture(0)  # For Webcam\n",
    "cap = cv2.VideoCapture(\"bikes.mp4.crdownload\") # for video\n",
    "cap.set(3, 1280)\n",
    "cap.set(4, 720)\n",
    "\n",
    "model = YOLO(\"path/to/your/yolov8l.pt\")\n",
    "\n",
    "classNames = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
    "              \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\",\n",
    "              \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\",\n",
    "              \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\",\n",
    "              \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\",\n",
    "              \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\",\n",
    "              \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"sofa\", \"pottedplant\", \"bed\",\n",
    "              \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\",\n",
    "              \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\",\n",
    "              \"teddy bear\", \"hair drier\", \"toothbrush\", \"note book\", \"ear bards\", \"eye ware\",\"pant\",\"cloth\",\"hand\", \"mouth\",\"nose\" \n",
    "              ]\n",
    "\n",
    "prev_frame_time = 0\n",
    "new_frame_time = 0\n",
    "\n",
    "while True:\n",
    "    new_frame_time = time.time()\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        print(\"Failed to capture frame from webcam. Exiting...\")\n",
    "        break\n",
    "\n",
    "    results = model(img, stream=True)\n",
    "    for r in results:\n",
    "        boxes = r.boxes\n",
    "        for box in boxes:\n",
    "            # Bounding Box\n",
    "            x1, y1, x2, y2 = box.xyxy[0]\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "            #cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 255), 3)  # Draw bounding box\n",
    "            w, h = x2 - x1, y2 - y1\n",
    "            cvzone.cornerRect(img, (x1, y1, w, h))\n",
    "            # Confidence\n",
    "            conf = math.ceil((box.conf[0] * 100)) / 100\n",
    "            # Class Name\n",
    "            cls = int(box.cls[0])\n",
    "\n",
    "            cvzone.putTextRect(img, f'{classNames[cls]} {conf}', (max(0, x1), max(35, y1)), scale=1, thickness=1)\n",
    "\n",
    "    fps = 1 / (new_frame_time - prev_frame_time)\n",
    "    prev_frame_time = new_frame_time\n",
    "    print(fps)\n",
    "\n",
    "    cv2.imshow(\"Image\", img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99eab37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
